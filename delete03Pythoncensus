import ee, pdb, os
ee.Authenticate() 


#https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html
#https://www.census.gov/programs-surveys/geography/technical-documentation/complete-technical-documentation/tiger-geo-line.2021.html#list-tab-240499709





#build census data csv
import pandas as pd
import numpy as np
import requests, csv
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
#set variables
year = 2021
dsource = 'acs'
dname = 'acs5'
#cols = 'NAME,B15003_001E'
cols = 'B06012_001E,B06012_002E,B19013_001E,B25064_001E,B23025_001E,B23025_002E,B23025_005E,B23025_007E,B23018_001E,B19301_001E,B15003_001E,B15003_017E,B15003_018E,B15003_019E,B15003_020E,B15003_021E,B15003_022E,B15003_023E,B15003_024E,B15003_025E,B17011_001E,B19051_001E,B19051_002E,B19051_003E,B19055_002E,B19055_003E,B19056_001E,B19056_002E,B19057_002E,B19058_002E,B19083_001E,B19113_001E,B01003_001E,B02001_001E,B02001_002E,B02001_003E,B02001_004E,B02001_005E,B02001_006E,B02001_007E,B02001_008E,B19123_002E,B19123_003E'
state = '06'
county = '*'
block_group = '*'
#list of geographies: https://api.census.gov/data/2022/acs/acs5/geography.html
dcode = '2,12'
keyfile = 'apikey.txt'
outfile = 'nosipopca2021.txt'

base_url = f'https://api.census.gov/data/{year}/{dsource}/{dname}'

with open(keyfile) as key:
    api_key = key.read().strip()

data_url = f'{base_url}?get={cols}&for=block%20group:{block_group}&in=state:{state}&in=county:{county}&in=tract:*&key={api_key}'
response = requests.get(data_url)
popdata = response.json()
#for record in popdata:
#    print(record)

with open(outfile, 'w', newline='') as writefile:
    writer = csv.writer(writefile, quoting = csv.QUOTE_ALL, delimiter = ',')
    writer.writerows(popdata)
tempdf=pd.read_csv('nosipopca2021.txt', converters={'county': str, 'tract': str})
#Some data are not available at the census blockgroup level
#Identify unavailable data
res = tempdf.columns[tempdf.isnull().all(0)]
res

#Some data are not available at the census blockgroup level

#grab unavailable columns for census tract level

year = 2021
dsource = 'acs'
dname = 'acs5'
#cols = 'NAME,B15003_001E''
cols = 'B06012_001E,B06012_002E,B23018_001E,B19083_001E'
county = '*'
tract = '*'
#list of geographies: https://api.census.gov/data/2022/acs/acs5/geography.html
dcode = '2,12'
keyfile = 'apikey.txt'
outfile = 'nosimissing-tract.txt'

base_url = f'https://api.census.gov/data/{year}/{dsource}/{dname}'
data_url = f'{base_url}?get={cols}&for=tract:{tract}&in=state:{state}&in=county:{county}&in=tract:*&key={api_key}'
response = requests.get(data_url)
popdata = response.json()
#for record in popdata:
#    print(record)

with open(outfile, 'w', newline='') as writefile:
    writer = csv.writer(writefile, quoting = csv.QUOTE_ALL, delimiter = ',')
    writer.writerows(popdata)
missing=pd.read_csv('nosimissing-tract.txt', converters={'county': str, 'tract': str})
missing

#replace NAs with tract level data
df = tempdf.merge(missing, how = 'left', on = 'tract')
df.dropna(axis = 1, how = 'all')
df.drop(['state_y', 'county_y', 'B06012_001E_x', 'B06012_002E_x', 'B23018_001E_x', 'B19083_001E_x'], axis = 1, inplace = True)

#rename columns
colnames = ['Median_household_income','Median_gross_rent','Total__Population_16_and_over','Total_In_labor_force','Total_In_labor_force_Civilian_labor_force_Unemployed','Total_Not_in_labor_force','Per_capita_income_in_the_past_12_months_(in_2021_inflation-adjusted_dollars)','Total__Population_25_and_over','Total_Regular_high_school_diploma','Total_GED_or_alternative_credential','Total_Some_college_less_than_1_year','Total_Some_college_1_or_more_years_no_degree','Associates','Bachelors','Masters','Professional','Doctorate','Aggregate_income_deficit_in_the_past_12_months','Total__Households','Total_With_earnings','Total_No_earnings','Total_With_Social_Security_income','Total_No_Social_Security_income','Total__Households','Total_With_Supplemental_Security_Income_(SSI)','Total_With_public_assistance_income','Total_With_cash_public_assistance_or_Food_Stamps/SNAP','Median_family_income_in_the_past_12_months_(in_2021_inflation-adjusted_dollars)','Total_Population','Total__Race','Total_White_alone','Total_Black_or_African_American_alone','Total_American_Indian_and_Alaska_Native_alone','Total_Asian_alone','Total_Native_Hawaiian_and_Other_Pacific_Islander_alone','Total_Some_other_race_alone','Total_Two_or_more_races','Total_With_cash_public_assistance_income_or_households_receiving_Food_Stamps/SNAP_benefits_in_the_past_12_months','Total_No_cash_public_assistance_income_or_household_Food_Stamps/SNAP_benefits_in_the_past_12_months','state', 'county','tract', 'blockgroup','TotalPopforPoverty', 'PopinPoverty','Working_hours','Gini_index']
df.columns = colnames
df['GEOID']= df['state'].astype(str) + df['county'].astype(str) + df['tract'].astype(str) + df['blockgroup'].astype(str)
df.drop(['county', 'tract', 'state', 'blockgroup'], axis = 1, inplace = True)
df['GEOID'] = pd.to_numeric(df.GEOID)
#make a couple new columns: percent with a degree, place code (equivalent to FIPS), percent households below poverty
df = df.mask(df < 0)
df['percent_degree']= (df['Bachelors']+df['Masters']+df['Professional']+df['Doctorate'])/df['Total__Population_25_and_over']


df["Poverty"] = df['PopinPoverty']/df['TotalPopforPoverty']
df['Unemployed'] = df['Total_In_labor_force_Civilian_labor_force_Unemployed']/df['Total_In_labor_force']
df['Food_assistance'] = df['Total_With_cash_public_assistance_income_or_households_receiving_Food_Stamps/SNAP_benefits_in_the_past_12_months']/(df['Total_With_cash_public_assistance_income_or_households_receiving_Food_Stamps/SNAP_benefits_in_the_past_12_months'] + df['Total_No_cash_public_assistance_income_or_household_Food_Stamps/SNAP_benefits_in_the_past_12_months'])


#drop redundant columns 
df.drop(['Bachelors', 'Masters', 'Professional', 'Doctorate', 'PopinPoverty', 'TotalPopforPoverty', 'Total_In_labor_force', 'Total_In_labor_force_Civilian_labor_force_Unemployed', 'Total__Population_16_and_over', 'Total_Not_in_labor_force', 'Total_No_cash_public_assistance_income_or_household_Food_Stamps/SNAP_benefits_in_the_past_12_months', 'Total_With_cash_public_assistance_income_or_households_receiving_Food_Stamps/SNAP_benefits_in_the_past_12_months'], axis = 1, inplace = True)
df.to_csv('nosicensusdf.csv')

df = pd.read_csv('nosicensusdf.csv')

#add in shapefile data
import geopandas as gpd
#camap = gpd.read_file('C:/Users/ariver10/Documents/nosi/tl_2023_06_bg/tl_2023_06_bg.shp')
#camap['GEOID'] = pd.to_numeric(camap.GEOID)
temp = camap['geometry'].to_crs("ESRI:102003")
camap['shapearea'] = temp.area
camap['shapelength'] = temp.length

#merge shapefile and df
dfmap = df.merge(camap, how = 'left', on = 'GEOID')

camap.head()

dfmap= dfmap[['ALAND', 'AWATER', 'Median_household_income', 'Median_gross_rent',
       'Per_capita_income_in_the_past_12_months_(in_2021_inflation-adjusted_dollars)',
       'Total__Population_25_and_over', 'Total_Regular_high_school_diploma',
       'Total_GED_or_alternative_credential',
       'Total_Some_college_less_than_1_year',
       'Total_Some_college_1_or_more_years_no_degree', 'Associates',
       'Aggregate_income_deficit_in_the_past_12_months', 'Total__Households',
       'Total_With_earnings', 'Total_No_earnings',
       'Total_With_Social_Security_income', 'Total_No_Social_Security_income',
       'Total__Households.1', 'Total_With_Supplemental_Security_Income_(SSI)',
       'Total_With_public_assistance_income',
       'Total_With_cash_public_assistance_or_Food_Stamps/SNAP',
       'Median_family_income_in_the_past_12_months_(in_2021_inflation-adjusted_dollars)',
       'Total_Population', 'Total__Race', 'Total_White_alone',
       'Total_Black_or_African_American_alone',
       'Total_American_Indian_and_Alaska_Native_alone', 'Total_Asian_alone',
       'Total_Native_Hawaiian_and_Other_Pacific_Islander_alone',
       'Total_Some_other_race_alone', 'Total_Two_or_more_races',
       'Working_hours', 'Gini_index', 'percent_degree', 'Poverty',
       'Unemployed', 'Food_assistance', 'BLKGRPCE', 'COUNTYFP', 'FUNCSTAT', 'GEOID', 'GEOIDFQ','INTPTLAT', 'INTPTLON','MTFCC',
        'NAMELSAD','shapearea', 'shapelength' ,'STATEFP', 'TRACTCE', 'geometry']]

#note we are missing columns objectid, systemindex
dfmap.to_csv('nosicensusshape.csv', index_label = 'systemindex')



#in google earth engine online, injest nosicensusshape

ee.Initialize()
from geeViz import taskManagerLib

#-----------------------------------------------------
#			Options
#-----------------------------------------------------
export_distances = False
export_nlcd = True
export_canopy_cover = True
export_canopy_area = True
export_geos = True


# Tiger Urban Areas
#This was generated in the GEE scripts
tiger = ee.FeatureCollection('projects/ee-ajnaram/assets/UrbanAreasCV')

# CBGs with UHI/LST values and City Name
# Get these from this script: https://code.earthengine.google.com/5f05c717099f6e6b564afa13b2f9c161, (Gmail from TC Feb. 25 2021)
lst_cbg = ee.FeatureCollection('projects/ee-ajnaram/assets/CVUHI_cb_vf_All').distinct(['.geo', 'geoid', 'Urban_LSTsummer']) # There are some duplicates\
cbg_tcc = lst_cbg.select(['NAME','geoid','aland','awater','Ar_pixel'])
lst_cbg = lst_cbg.select(['NAME','geoid'])

## Next step is to build my own Central Valley blockgroup Feature collection
idea: select by county (San Joaquin, Merced, Sacramento)

# Block Group Level - Income & Population - for all CA
incomePop = ee.FeatureCollection('projects/ee-ajnaram/assets/UrbanAreasCV')

# NLCD Land cover at 30m resolution
dataset = ee.ImageCollection('USGS/NLCD_RELEASES/2021_REL/NLCD')
nlcd2021 = dataset.filter(ee.Filter.eq('system:index', '2021')).first()
nlcd = nlcd2021.select('landcover')

# Earth Define Canopy cover at 1m resolution
# earthdefine image collection 
earthdefine = ee.ImageCollection('users/leahscampbell/contour/tnc/urbantree/EarthDefine').mosaic()\
  .unmask(ee.Image(0),False).updateMask(nlcd.neq(11)).clip(tiger)


# Size parameters for filtering clipped geometries (see more info below)
canopyDataMaskMinSize = 15000
canopyDataMask2MinSize = 200000
canopyDataMaskPortion = 0.01
canopyDataMaskRatio = 0.9
#----------------------------

#----------------------------------------------------------
#           Initialize List of Cities
#----------------------------------------------------------
# List of cities to loop through
areaList = lst_cbg.aggregate_histogram('NAME').keys().getInfo()
areaNameStrings = []
for name in areaList:
	cleanName = name.split(',')[0]\
              .replace(' ','')\
              .replace(')','')\
              .replace('(','')\
              .replace('.','')
	areaNameStrings.append(cleanName)
areaDict = {areaList[i]: areaNameStrings[i] for i in range(len(areaList))}

areaList

#----------------------------------------------------------
#           Calculate and Export Distances
#----------------------------------------------------------
if export_distances:
    print('Export Distances')
    def add_distance(feature):
        centr = feature.geometry().centroid()
        urb_centr = ee.Feature(tiger.filter(ee.Filter.eq('NAME10', feature.get('NAME'))).first()).geometry().centroid()
        dist_urb = ee.Number(centr.distance(urb_centr, 5))
        return feature.set({'Dist_urbCenter': dist_urb})

    cityDistances = []
    for cityName in areaList:
        print('Distances:', cityName)

        cityCBGs = lst_cbg.filter(ee.Filter.eq('NAME', cityName))

        distances = cityCBGs.map(add_distance)

        cityDistances.append(distances)

    exportDistances = ee.FeatureCollection(cityDistances).flatten()

# Set geometries to null so that .geo column doesn't make the file huge.
    exportDistances = exportDistances.map(lambda cbg: cbg.setGeometry(None))

#    t = ee.batch.Export.table.toAsset({'collection': exportDistances, 'description': 'Distances_Table_toStorage',  'assetId':'users/ajnaram/Distances_CBG_Table_All', 'fileFormat': 'CSV'})
#    t.start()
#Export the image sample feature collection to Drive as a CSV file.
    task = ee.batch.Export.table.toDrive(collection=exportDistances, description='Distances_table', fileFormat='CSV')
    task.start()




#----------------------------------------------------------
#           Calculate and Export NLCD Percentages
#----------------------------------------------------------
if export_nlcd:
	print('Export NLCD')

	# Add NLCD Classes & Canopy Percent
	nlcdImage = ee.Image(nlcd.eq(21)).rename('NLCD_Dev_OpenSpace_Perc')\
		.addBands(ee.Image(nlcd.eq(22)).rename('NLCD_Dev_LowIntensity_Perc'))\
		.addBands(ee.Image(nlcd.eq(23)).rename('NLCD_Dev_MedIntensity_Perc'))\
		.addBands(ee.Image(nlcd.eq(24)).rename('NLCD_Dev_HighIntensity_Perc'))\
		.addBands(ee.Image(nlcd.lt(21).Or(nlcd.gt(24))).rename('NLCD_Other_Perc'))

	exportNLCD = nlcdImage.reduceRegions(**{\
		'collection': lst_cbg,
		'reducer': ee.Reducer.mean(), 
		'scale': 30, 
		'crs': None, 
		'crsTransform': None, 
		'tileScale': 1})

	# Set geometries to null so that .geo column doesn't make the file huge.
	exportNLCD = exportNLCD.map(lambda cbg: cbg.setGeometry(None))

	task = ee.batch.Export.table.toDrive(collection= exportNLCD, description= 'NLCD_Percent_Table_toAsset',fileFormat='CSV')
	task.start()



#----------------------------------------------------------
#           Calculate and Export Tree Canopy Cover
#----------------------------------------------------------
# These are exported by city because they take a while...and it fails if you do too many CBGs at once.

if export_canopy_cover:
	print('Export Canopy Cover')

	# Pixel area, for use in canopy area calculation
	Im_Area=ee.Image.pixelArea().updateMask(nlcd.neq(11)).clip(tiger)

	for cityName in areaList:
		print('Canopy Cover:', cityName)
		cityPolys = cbg_tcc.filter(ee.Filter.eq('NAME', cityName))
		omit = ['060819901000', '060419901000'] # These are large CBGs that are completely water in the bay area - were making it crash.
		cityPolys = cityPolys.filter(ee.Filter.inList('geoid', omit).Not())

		if cityPolys.size().getInfo() > 0:
			cityPolys = cityPolys.map(lambda poly: poly.set('mean', -99999))

			# Add Canopy Percentage
			canopyPercent = earthdefine.reduceRegions(**{\
				'collection': cityPolys,
	            'reducer': ee.Reducer.mean(), 
	            'scale': 1, 
	            'crs': None, 
	            'crsTransform': None, 
	            'tileScale': 8})
			#props = canopyPercent.first().propertyNames()
			#canopyPercent = canopyPercent.select(props, props.replace('mean','Canopy_Percent'))#.replace('sum','CanopyData_Area_1m'))

#			# Set geometries to null so that .geo column doesn't make the file huge.
			canopyPercent = canopyPercent.map(lambda cbg: cbg.setGeometry(None))

			# Export
			outStr = areaDict[cityName]
			print(outStr, 'canopyPercent')
			t = ee.batch.Export.table.toDrive(
	            collection = canopyPercent, 
	            description = "Canopy_Percent_Table_toStorage", 
	            fileFormat = 'CSV')
			t.start()
			# t = ee.batch.Export.table.toAsset(**{\
			#     'collection': canopyPercent, 
			#     'description': 'Canopy_Percent_'+outStr+'_Table_toAsset', 
			#     'assetId': 'users/leahscampbell/contour/tnc/urbantree/canopyPercent_Tables/Canopy_Percent_'+outStr+'_Table'})
			# t.start()



# To check for failed tasks:
# tasks = ee.data.getTaskList()[0:211]
# status = [task['state'] for task in tasks]
# name = [task['description'] for task in tasks]
# for i, state in enumerate(status):
#   if state == 'FAILED':
#     print('FAILED: '+name[i])

#----------------------------------------------------------
#           Calculate and Export Canopy Data Areas
#----------------------------------------------------------
# These are exported by city because they take a while...and it fails if you do too many CBGs at once.

# In order to export Clipped Geometries, this must be exported to Asset.
# The 1m Area and 30m Area are used for this - to find very small areas within CBGs that should be eliminated.
# This is kind of a hack, but we are limited by the way GEE deals with pixels along a polygon boundary that intersects pixels.

if export_canopy_area:
	print('Export Canopy Area')

	# Pixel area, for use in canopy area calculation
	Im_Area=ee.Image.pixelArea().updateMask(nlcd.neq(11)).clip(tiger)

	for cityName in areaList:

		cityPolys = cbg_tcc.filter(ee.Filter.eq('NAME', cityName))
		cityPolys = cityPolys.filter(ee.Filter.inList('geoid', omit).Not())

		if cityPolys.size().getInfo() > 0:
			cityPolys = cityPolys.map(lambda poly: poly.set('sum', -9999))

			outStr = areaDict[cityName]

			# Add Area Covered by Earth Define Data (1 meter scale)
			canopyArea = Im_Area.reduceRegions(**{\
				'collection': cityPolys,
	            'reducer': ee.Reducer.sum(), 
	            'scale': 1, 
	            'crs': None, 
	            'crsTransform': None,  
	            'tileScale': 4})
			props = canopyArea.first().propertyNames()
			canopyArea = canopyArea.select(props, props.replace('sum','CanopyData_Area_1m').replace('Ar_pixel','CanopyData_Area_30m'))

			# Export
			t = ee.batch.Export.table.toDrive(
			    collection = canopyArea, 
			    description = 'Canopy_Area_'+outStr+'_Table_toAsset')
			t.start()
            
            

# To check for failed tasks:
#tasks = ee.data.getTaskList()[0:211]
#status = [task['state'] for task in tasks]
#name = [task['description'] for task in tasks]
#for i, state in enumerate(status):
#   if state == 'FAILED':
#     print('FAILED: '+name[i])

#----------------------------------------------------------
#           Calculate and Export Tree Canopy Cover
#----------------------------------------------------------
# These are exported by city because they take a while...and it fails if you do too many CBGs at once.

if export_canopy_cover:
	print('Export Canopy Cover')

	# Pixel area, for use in canopy area calculation
	Im_Area=ee.Image.pixelArea().updateMask(nlcd.neq(11)).clip(tiger)

	for cityName in areaList:
		print('Canopy Cover:', cityName)
		cityPolys = cbg_tcc.filter(ee.Filter.eq('NAME', cityName))
		omit = ['060819901000', '060419901000'] # These are large CBGs that are completely water in the bay area - were making it crash.
		cityPolys = cityPolys.filter(ee.Filter.inList('geoid', omit).Not())

		if cityPolys.size().getInfo() > 0:
			cityPolys = cityPolys.map(lambda poly: poly.set('mean', -99999))

			# Add Canopy Percentage
			canopyPercent = earthdefine.reduceRegions(**{\
				'collection': cityPolys,
	            'reducer': ee.Reducer.mean(), 
	            'scale': 1, 
	            'crs': None, 
	            'crsTransform': None, 
	            'tileScale': 8})
			#props = canopyPercent.first().propertyNames()
			#canopyPercent = canopyPercent.select(props, props.replace('mean','Canopy_Percent'))#.replace('sum','CanopyData_Area_1m'))

#			# Set geometries to null so that .geo column doesn't make the file huge.

			# Export
			outStr = areaDict[cityName]
			print(outStr, 'canopyPercent')
			tempname ='projects/ee-ajnaram/assets/Canopy_Percent_'+outStr+'_Table_toAsset'          
			task = ee.batch.Export.table.toAsset(
	            collection = canopyPercent, 
	            description = "Canopy_Percent_Table_toStorage", 
	            assetId = tempname)
			task.start()
			# t = ee.batch.Export.table.toAsset(**{\
			#     'collection': canopyPercent, 
			#     'description': 'Canopy_Percent_'+outStr+'_Table_toAsset', 
			#     'assetId': 'users/leahscampbell/contour/tnc/urbantree/canopyPercent_Tables/Canopy_Percent_'+outStr+'_Table'})
			# t.start()


#----------------------------------------------------------
#           Calculate and Export Canopy Data Areas
#----------------------------------------------------------
# These are exported by city because they take a while...and it fails if you do too many CBGs at once.

# In order to export Clipped Geometries, this must be exported to Asset.
# The 1m Area and 30m Area are used for this - to find very small areas within CBGs that should be eliminated.
# This is kind of a hack, but we are limited by the way GEE deals with pixels along a polygon boundary that intersects pixels.

if export_canopy_area:
	print('Export Canopy Area')

	# Pixel area, for use in canopy area calculation
	Im_Area=ee.Image.pixelArea().updateMask(nlcd.neq(11)).clip(tiger)

	for cityName in areaList:

		cityPolys = cbg_tcc.filter(ee.Filter.eq('NAME', cityName))
		cityPolys = cityPolys.filter(ee.Filter.inList('geoid', omit).Not())

		if cityPolys.size().getInfo() > 0:
			cityPolys = cityPolys.map(lambda poly: poly.set('sum', -9999))

			outStr = areaDict[cityName]

			# Add Area Covered by Earth Define Data (1 meter scale)
			canopyArea = Im_Area.reduceRegions(**{\
				'collection': cityPolys,
	            'reducer': ee.Reducer.sum(), 
	            'scale': 1, 
	            'crs': None, 
	            'crsTransform': None,  
	            'tileScale': 4})
			props = canopyArea.first().propertyNames()
			canopyArea = canopyArea.select(props, props.replace('sum','CanopyData_Area_1m').replace('Ar_pixel','CanopyData_Area_30m'))

			# Export
			tempID =  'projects/ee-ajnaram/assets/Canopy_Area_'+outStr
			t = ee.batch.Export.table.toAsset(
			    collection = canopyArea, 
			    description = 'Canopy_Area_'+outStr+'_Table_toAsset', assetId = tempID)
			t.start()
            
            

# To check for failed tasks:
#tasks = ee.data.getTaskList()[0:211]
#status = [task['state'] for task in tasks]
#name = [task['description'] for task in tasks]
#for i, state in enumerate(status):
#   if state == 'FAILED':
#     print('FAILED: '+name[i])

tempname ='projects/ee-ajnaram/assets/Canopy_Percent_'+outStr+'_Table_toAsset'
tempname


#cat Canopy percentages and merge to census data
#download canopy percentage csv from google drive and put here
#alternatively import straight from google drive
import glob
import os
import pandas as pd
all_files = glob.glob("C:/Users/ariver10/Documents/nosi/canopypercent/*.csv")
canopy = pd.concat((pd.read_csv(f) for f in all_files))
canopy = canopy.drop_duplicates()
canopy.drop(['system:index', 'Ar_pixel', 'aland', 'awater', '.geo', 'Canopy_Percent'], axis = 1, inplace = True)

canopymap = canopy.merge(dfmap, how = 'left', right_on = 'GEOID', left_on = 'geoid')
canopymap.drop_duplicates()

canopymap = canopymap.drop_duplicates(subset = ['geoid'])


canopymap.columns

canopymap.head()

canopymap.to_csv('canopymap.csv', index_label = 'index')

gdf = gpd.GeoDataFrame(canopymap, geometry="geometry")

#gdf = gdf[gdf['Median_household_income'].notna()]
#need to clip rural areas, check for inclusion of all census blocks in urban areas
stk = gdf[gdf['NAME'] == "Stockton, CA"]

stk.plot(column = 'mean')











